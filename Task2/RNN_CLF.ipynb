{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Classification by RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-13T09:05:57.228764Z",
     "start_time": "2020-02-13T09:05:50.567963Z"
    }
   },
   "outputs": [],
   "source": [
    "# library\n",
    "import gensim\n",
    "import pandas as pd\n",
    "# import nltk\n",
    "# import string\n",
    "# import re\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "# import matplotlib.pyplot as plt\n",
    "# %matplotlib inline\n",
    "import torch.utils.data as Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-13T09:09:13.001347Z",
     "start_time": "2020-02-13T09:07:23.204748Z"
    }
   },
   "outputs": [],
   "source": [
    "word2vec = gensim.models.KeyedVectors.load_word2vec_format(\"GoogleNews-vectors-negative300.bin.gz\", binary=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-13T09:09:17.567887Z",
     "start_time": "2020-02-13T09:09:15.477392Z"
    }
   },
   "outputs": [],
   "source": [
    "data=pd.read_table('train.tsv',sep='\\t')\n",
    "# texts=data['Phrase'].tolist()\n",
    "data_y=data[\"Sentiment\"]\n",
    "data_y=np.array(data_y)\n",
    "N=len(data_y)\n",
    "\n",
    "# words_ls=pd.read_table('words_ls.txt',header=None)[0]\n",
    "# words_ls=[eval(words) for words in words_ls]\n",
    "\n",
    "stopwords = ('-',',','.',';',':',\"'\",'?')  # 停词\n",
    "texts=data['Phrase'].tolist()\n",
    "words_ls = []\n",
    "i=1\n",
    "for text in texts:\n",
    "    remove = str.maketrans('','',string.punctuation) \n",
    "    text = text.translate(remove)\n",
    "    if i%100 == 0:\n",
    "        print(\"当前已完成%.2f\" % (i*100/156060)+'%',end='\\r')\n",
    "    words = [word for word in text.split() if (word not in stopwords)]\n",
    "    words_ls.append(words)\n",
    "    i+=1\n",
    "\n",
    "word_maxlen=0\n",
    "for words in words_ls:\n",
    "    word_maxlen=max(word_maxlen,len(words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-13T09:09:30.376766Z",
     "start_time": "2020-02-13T09:09:30.370781Z"
    }
   },
   "outputs": [],
   "source": [
    "# Hyper Parameters\n",
    "EPOCH = 2 \n",
    "BATCH_SIZE = 200\n",
    "TIME_STEP = word_maxlen      # rnn 时间步数 / 图片高度\n",
    "\n",
    "VEC_LEN = 300\n",
    "INPUT_SIZE = VEC_LEN     # rnn 每步输入值 / 图片每行像素\n",
    "LR = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-13T10:12:34.785843Z",
     "start_time": "2020-02-13T10:12:34.778828Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNN(\n",
      "  (rnn): LSTM(300, 64, batch_first=True)\n",
      "  (out): Linear(in_features=64, out_features=5, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RNN, self).__init__()\n",
    "\n",
    "        self.rnn = nn.LSTM(\n",
    "            input_size=INPUT_SIZE,      # 图片每行的数据像素点\n",
    "            hidden_size=64,     # rnn hidden unit\n",
    "            num_layers=1,       # 有几层 RNN layers\n",
    "            batch_first=True,   # input & output 会是以 batch size 为第一维度的特征集 e.g. (batch, time_step, input_size)\n",
    "        )\n",
    "\n",
    "        self.out = nn.Linear(64, 5)    # 输出层\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape (batch, time_step, input_size)\n",
    "        # r_out shape (batch, time_step, output_size)\n",
    "        # h_n shape (n_layers, batch, hidden_size)   LSTM 有两个 hidden states, h_n 是分线, h_c 是主线\n",
    "        # h_c shape (n_layers, batch, hidden_size)\n",
    "        r_out, (h_n, h_c) = self.rnn(x, None)   # None 表示 hidden state 会用全0的 state\n",
    "\n",
    "        # 选取最后一个时间点的 r_out 输出\n",
    "        # 这里 r_out[:, -1, :] 的值也是 h_n 的值\n",
    "        out = self.out(r_out[:, -1, :])\n",
    "        return out\n",
    "\n",
    "rnn = RNN()\n",
    "print(rnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-13T10:20:52.649678Z",
     "start_time": "2020-02-13T10:20:52.634706Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0., 0., 0.],\n",
       "        [0., 0., 0.]]])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.zeros([1,2,3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-13T10:25:19.014229Z",
     "start_time": "2020-02-13T10:25:18.990295Z"
    }
   },
   "source": [
    "## WV to Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-13T10:42:09.353864Z",
     "start_time": "2020-02-13T10:42:09.340476Z"
    }
   },
   "outputs": [],
   "source": [
    "def wv_to_tensor_ls(inds,v_length=VEC_LEN,WV=word2vec):\n",
    "    # output is a list of tensors\n",
    "    l=len(inds)\n",
    "    out=[]\n",
    "    for i in range(l):\n",
    "        words=words_ls[inds[i]]\n",
    "        n=len(words)\n",
    "        \n",
    "        if n>0: \n",
    "            wv=np.zeros([1,n,v_length])\n",
    "            try:\n",
    "                wv[0,:,:]=WV[words]\n",
    "            except KeyError:\n",
    "                for h in range(n):\n",
    "                    try:\n",
    "                        wv[0,h,:]=WV[words[h]].reshape(1,v_length)\n",
    "                    except KeyError:\n",
    "                        wv[0,h,:]=np.random.randn(1,v_length)/10\n",
    "                        # 到此 wordvec的type还是np.array, need to convert to torch.tensor\n",
    "        else: #(n==0) 随机赋值\n",
    "            wv=np.random.randn(1,1,VEC_LEN)/100\n",
    "        wv=torch.from_numpy(wv).to(torch.float32)\n",
    "        out.append(wv)\n",
    "        \n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train & Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-13T11:25:39.002419Z",
     "start_time": "2020-02-13T11:25:38.989840Z"
    }
   },
   "outputs": [],
   "source": [
    "def train_rnn(EPOCH = 2 ,BATCH_SIZE = 100,LR = 0.01,wv=word2vec):\n",
    "    \n",
    "    rnn = RNN()\n",
    "    optimizer = torch.optim.Adam(rnn.parameters(), lr=LR)   # optimize all parameters\n",
    "    loss_func = nn.CrossEntropyLoss()   # the target label is not one-hotted\n",
    "\n",
    "    train_inds=random.sample(range(N),np.int(np.floor(N*0.8)))\n",
    "    test_inds=list(set(range(N))-set(train_inds))\n",
    "\n",
    "    mat=np.concatenate((np.arange(N).reshape(N,1),data_y.reshape(N,1)),axis=1)\n",
    "    train_loader = Data.DataLoader(dataset=mat[train_inds,:], batch_size=BATCH_SIZE, shuffle=True)\n",
    "    test_x=wv_to_tensor_ls(inds=mat[test_inds,0],WV=wv) # it's a list\n",
    "    test_y=torch.from_numpy(mat[test_inds,1])\n",
    "\n",
    "    for epoch in range(EPOCH):\n",
    "        for step, batch_data in enumerate(train_loader):   # gives batch data, normalize x when iterate train_loader\n",
    "            b_x_ind = batch_data[:,0]  # batch x\n",
    "            b_y = batch_data[:,1]   # batch y\n",
    "            b_x = wv_to_tensor_ls(inds=b_x_ind,WV=wv) # it's a list, when training, iteration is needed.\n",
    "            \n",
    "            output=torch.FloatTensor()\n",
    "            for x in b_x:\n",
    "                output = torch.cat([output, rnn(x)], dim=0) \n",
    "                \n",
    "            loss = loss_func(output, b_y)   # cross entropy loss\n",
    "            optimizer.zero_grad()           # clear gradients for this training step\n",
    "            loss.backward()                 # backpropagation, compute gradients\n",
    "            optimizer.step()                # apply gradients\n",
    "\n",
    "            if step % 200 == 199:\n",
    "                test_output = torch.FloatTensor()\n",
    "                for x in test_x:\n",
    "                    test_output = torch.cat([test_output, rnn(x)], dim=0)\n",
    "                pred_y = torch.max(test_output, 1)[1].data.squeeze()\n",
    "                accuracy = (pred_y == test_y).sum().item() / float(test_y.size(0))\n",
    "                print('Epoch: ',epoch+1, '| Step: ', step+1, '| train loss: %.4f' % loss.data, '| test accuracy: %.2f' % accuracy)\n",
    "    return (rnn,accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-13T11:22:02.904486Z",
     "start_time": "2020-02-13T11:22:02.891560Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x1=np.random.randn(1,10,300)\n",
    "x1=torch.from_numpy(x1).to(torch.float32)\n",
    "output=rnn(x1)\n",
    "output = torch.cat([output, rnn(x1)], dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-13T11:22:10.568126Z",
     "start_time": "2020-02-13T11:22:10.558139Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.5919,  0.6655,  0.2829, -0.0249, -0.2737],\n",
       "        [-0.5919,  0.6655,  0.2829, -0.0249, -0.2737]], grad_fn=<CatBackward>)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "tensor([1, 1])"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "tensor([1, 1])"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output\n",
    "torch.max(output, 1)[1].data.squeeze()\n",
    "torch.max(output, 1)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-13T11:50:21.306838Z",
     "start_time": "2020-02-13T11:25:41.433758Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  1 | Step:  200 | train loss: 0.9738 | test accuracy: 0.62\n",
      "Epoch:  1 | Step:  400 | train loss: 1.0327 | test accuracy: 0.63\n",
      "Epoch:  1 | Step:  600 | train loss: 0.9101 | test accuracy: 0.63\n",
      "Epoch:  1 | Step:  800 | train loss: 0.9056 | test accuracy: 0.64\n",
      "Epoch:  1 | Step:  1000 | train loss: 0.8129 | test accuracy: 0.64\n",
      "Epoch:  1 | Step:  1200 | train loss: 0.8013 | test accuracy: 0.64\n",
      "Epoch:  2 | Step:  200 | train loss: 0.8466 | test accuracy: 0.64\n",
      "Epoch:  2 | Step:  400 | train loss: 0.7765 | test accuracy: 0.65\n",
      "Epoch:  2 | Step:  600 | train loss: 0.8576 | test accuracy: 0.65\n",
      "Epoch:  2 | Step:  800 | train loss: 0.8357 | test accuracy: 0.65\n",
      "Epoch:  2 | Step:  1000 | train loss: 0.8024 | test accuracy: 0.65\n",
      "Epoch:  2 | Step:  1200 | train loss: 0.7829 | test accuracy: 0.66\n"
     ]
    }
   ],
   "source": [
    "rnn, acc=train_rnn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-13T11:56:29.483693Z",
     "start_time": "2020-02-13T11:56:29.479673Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6556"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "round(acc,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-13T11:57:21.500907Z",
     "start_time": "2020-02-13T11:57:21.411440Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\torch\\serialization.py:360: UserWarning: Couldn't retrieve source code for container of type RNN. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    }
   ],
   "source": [
    "torch.save(rnn, 'rnn-g300.pkl')  # save entire net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (tf2.0)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
